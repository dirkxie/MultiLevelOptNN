{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.models.image.cifar10 import cifar10\n",
    "import cifar10\n",
    "import cifar10_mod\n",
    "#import cifar10_input\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/util')\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "#from extract_weights import extract_weights\n",
    "from Net2Net import Net2Net_class\n",
    "from extract_weights import extract_weights\n",
    "from train_load_net import train_load_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_string('train_dir_new', '/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train_new',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar10.distorted_inputs()\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = cifar10.inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = cifar10.loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = cifar10.train(loss, global_step)\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Start running operations on the Graph.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        log_device_placement=FLAGS.log_device_placement))\n",
    "    sess.run(init)\n",
    "\n",
    "\n",
    "    # 1. check if checkpoint exists\n",
    "    print('1. Check if checkpoint path exists:')\n",
    "    print('   FLAGS.train_dir is %s' % FLAGS.train_dir)\n",
    "    \n",
    "    if FLAGS.train_dir is not None:\n",
    "      # Restoring from the checkpoint file\n",
    "      print('   Checkpoint path found. Load checkpoint state:')\n",
    "      ckpt_state = tf.train.get_checkpoint_state(FLAGS.train_dir)\n",
    "      print(ckpt_state)\n",
    "      if ckpt_state is not None:\n",
    "        saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "      else:\n",
    "        print('   No existing checkpoint. Start a new session.')\n",
    "\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\n",
    "    \n",
    "    # 2. load current step, if checkpoint exist, will load ckpt step\n",
    "    cur_step = sess.run(global_step);    \n",
    "    print('2. Print check point information:')\n",
    "    print('   current step is %s' % cur_step)\n",
    "    \n",
    "    # print out variable shapes\n",
    "    print('   Shape of all trainable variables:')\n",
    "    for var in tf.trainable_variables():\n",
    "        print('     ', var.name)\n",
    "        print('     ', var.get_shape())\n",
    "            \n",
    "    # 3. start resume training\n",
    "    print('3. Start/Resume training:')\n",
    "    for step in xrange(cur_step, FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "      _, loss_value = sess.run([train_op, loss])\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "      if step % 10 == 0:\n",
    "        num_examples_per_step = FLAGS.batch_size\n",
    "        examples_per_sec = num_examples_per_step / duration\n",
    "        sec_per_batch = float(duration)\n",
    "\n",
    "        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                      'sec/batch)')\n",
    "        print (format_str % (datetime.now(), step, loss_value,\n",
    "                             examples_per_sec, sec_per_batch))\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "      # Save the model checkpoint periodically.\n",
    "      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_path, global_step=step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_modified_net(params):\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar10_mod.distorted_inputs()\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = cifar10_mod.inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = cifar10_mod.loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = cifar10_mod.train(loss, global_step)\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Start running operations on the Graph.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        log_device_placement=FLAGS.log_device_placement))\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir_new, sess.graph)\n",
    "    \n",
    "    # load weights from input params, extracted from other network\n",
    "    print('1. Reload and expand some weights')\n",
    "    for var in tf.trainable_variables():\n",
    "      var_name = var.name\n",
    "      var_shape = var.get_shape()\n",
    "      param_ndarray = params[var_name]\n",
    "      param_shape = np.shape(param_ndarray)\n",
    "      print('---------------')\n",
    "      print('     ', var_name)\n",
    "      print('     ', var_shape)\n",
    "      num_dim = np.shape(param_shape)[0]\n",
    "      \n",
    "      # check if expand\n",
    "      if (var_shape != param_shape and num_dim == 2):\n",
    "        print('      expand')\n",
    "        if (var_shape[0] != param_shape[0]):\n",
    "          expanded_ndarray = np.ndarray(shape=(param_shape[0]*2, param_shape[1]), dtype=float)\n",
    "          expanded_ndarray[0:param_shape[0], :] = param_ndarray\n",
    "          expanded_ndarray[param_shape[0]:param_shape[0]*2, :] = param_ndarray\n",
    "          assign_op = tf.assign(var, expanded_ndarray)\n",
    "          sess.run(assign_op)\n",
    "        elif (var_shape[1] != param_shape[1]):\n",
    "          expanded_ndarray = np.ndarray(shape=(param_shape[0], param_shape[1]*2), dtype=float)\n",
    "          expanded_ndarray[:, 0:param_shape[1]] = param_ndarray\n",
    "          expanded_ndarray[:, param_shape[1]:param_shape[1]*2] = param_ndarray\n",
    "          expanded_ndarray /= 2\n",
    "          assign_op = tf.assign(var, expanded_ndarray)\n",
    "          sess.run(assign_op)\n",
    "      elif (var_shape != param_shape and num_dim == 1):\n",
    "        print('      expand')\n",
    "        expanded_ndarray = np.ndarray(shape=(param_shape[0]*2), dtype=float)\n",
    "        expanded_ndarray[0:param_shape[0]] = param_ndarray\n",
    "        expanded_ndarray[param_shape[0]:param_shape[0]*2] = param_ndarray\n",
    "        expanded_ndarray /= 2\n",
    "        assign_op = tf.assign(var, expanded_ndarray)\n",
    "        sess.run(assign_op)\n",
    "      else: #direct copy\n",
    "        print('      direct copy')\n",
    "        assign_op = tf.assign(var, param_ndarray)\n",
    "        sess.run(assign_op)\n",
    "\n",
    "        \n",
    "    print('2. start/resume training')    \n",
    "    # start/resume training\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "      _, loss_value = sess.run([train_op, loss])\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "      if step % 10 == 0:\n",
    "        num_examples_per_step = FLAGS.batch_size\n",
    "        examples_per_sec = num_examples_per_step / duration\n",
    "        sec_per_batch = float(duration)\n",
    "\n",
    "        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                      'sec/batch)')\n",
    "        print (format_str % (datetime.now(), step, loss_value,\n",
    "                             examples_per_sec, sec_per_batch))\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "      # Save the model checkpoint periodically.\n",
    "      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir_new, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_path, global_step=step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_weights_old(ckpt_dir):\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar10.distorted_inputs()\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = cifar10.inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = cifar10.loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = cifar10.train(loss, global_step)\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Start running operations on the Graph.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        log_device_placement=FLAGS.log_device_placement))\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 1. check if checkpoint exists\n",
    "    print('1. Check if checkpoint path exists:')\n",
    "    print('   Checkpoint dir is %s' % ckpt_dir)\n",
    "    \n",
    "    if ckpt_dir is not None:\n",
    "      # Restoring from the checkpoint file\n",
    "      print('   Checkpoint path found. Load checkpoint state:')\n",
    "      ckpt_state = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "      print(ckpt_state)\n",
    "      if ckpt_state is not None:\n",
    "        saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "      else:\n",
    "        print('   No existing checkpoint. Start a new session.')\n",
    "\n",
    "    # 2. Extract weights\n",
    "    print('2. Extract weights:')\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for var in tf.trainable_variables():\n",
    "        var_name = var.name\n",
    "        var_shape = var.get_shape()\n",
    "        print('     ', var_name)\n",
    "        print('     ', var_shape)\n",
    "        params[var_name] = sess.run(var)\n",
    "    \n",
    "    sess.close()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "  cifar10.maybe_download_and_extract()\n",
    "  print ('downloaded')\n",
    "    \n",
    "  if tf.gfile.Exists(FLAGS.train_dir):\n",
    "    key_in = raw_input('Clear previously saved records? (y/n)\\n')\n",
    "    if (key_in == 'y'):\n",
    "      print(\"Deleting records...\\n\")\n",
    "      tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "      tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "  \n",
    "    # extract weights\n",
    "    key_in = raw_input('Train old or new network? (o/n)\\n')\n",
    "    if (key_in == 'o'):\n",
    "      print('====================================================')\n",
    "      print('Train old network')\n",
    "      print('====================================================')\n",
    "      train()\n",
    "    elif (key_in == 'n'):\n",
    "      print('====================================================')\n",
    "      print('Train new network')\n",
    "      print('====================================================')\n",
    "      print('======================extract weights======================')\n",
    "      #params = extract_weights('/tmp/cifar10_train')\n",
    "      params = extract_weights_old('/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train')\n",
    "      print('======================train on new network======================')\n",
    "      train_modified_net(params)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "<tensorflow.python.training.saver.Saver object at 0x10ee21f50>\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = FLAGS.train_dir\n",
    "\n",
    "test_graph = tf.Graph()\n",
    "\n",
    "with test_graph.as_default():\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "  # Get images and labels for CIFAR-10.\n",
    "  images, labels = cifar10.distorted_inputs()\n",
    "\n",
    "  # Build a Graph that computes the logits predictions from the\n",
    "  # inference model.\n",
    "  logits = cifar10.inference(images)\n",
    "\n",
    "  # Calculate loss.\n",
    "  loss = cifar10.loss(logits, labels)\n",
    "\n",
    "  # Build a Graph that trains the model with one batch of examples and\n",
    "  # updates the model parameters.\n",
    "  train_op = cifar10.train(loss, global_step)\n",
    "\n",
    "  # Create a saver.\n",
    "  saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "print(saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with test_graph.as_default():\n",
    "  params_dict = extract_weights(ckpt_dir, saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test adding neurons:\\n')\n",
    "\n",
    "weight1 = params_dict['conv1/weights:0']\n",
    "bias1 = params_dict['conv1/biases:0']\n",
    "weight2 = params_dict['conv2/weights:0']\n",
    "\n",
    "print('Original weight size')\n",
    "print(weight1.shape)\n",
    "print(bias1.shape)\n",
    "print(weight2.shape)\n",
    "print('\\n')\n",
    "\n",
    "new_width = 100\n",
    "\n",
    "N2N = Net2Net_class()\n",
    "\n",
    "stu_w1, stu_b1, stu_w2 = N2N.Net2Wider(weight1, bias1, weight2, new_width, verification=True)\n",
    "\n",
    "print('\\nWider weight')\n",
    "print(stu_w1.shape)\n",
    "print(stu_b1.shape)\n",
    "print(stu_w2.shape)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "deep_w, deep_b = N2N.Net2Deeper(weight1, verification=True)\n",
    "\n",
    "print('\\nDeeper weight')\n",
    "print(weight1.shape)\n",
    "print(deep_w.shape)\n",
    "print(deep_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_loaded_net(graph1, params):\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with graph1.as_default():\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Start running operations on the Graph.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        log_device_placement=FLAGS.log_device_placement))\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir_new, sess.graph)\n",
    "    \n",
    "    # load weights from input params, extracted from other network\n",
    "    print('1. Reload and expand some weights')\n",
    "    for var in tf.trainable_variables():\n",
    "      var_name = var.name\n",
    "      var_shape = var.get_shape()\n",
    "      param_ndarray = params[var_name]\n",
    "      param_shape = np.shape(param_ndarray)\n",
    "      print('---------------')\n",
    "      print('     ', var_name)\n",
    "      print('     ', var_shape)\n",
    "      num_dim = np.shape(param_shape)[0]\n",
    "      \n",
    "      # check if expand\n",
    "      if (var_shape != param_shape and num_dim == 2):\n",
    "        print('      expand')\n",
    "        if (var_shape[0] != param_shape[0]):\n",
    "          expanded_ndarray = np.ndarray(shape=(param_shape[0]*2, param_shape[1]), dtype=float)\n",
    "          expanded_ndarray[0:param_shape[0], :] = param_ndarray\n",
    "          expanded_ndarray[param_shape[0]:param_shape[0]*2, :] = param_ndarray\n",
    "          assign_op = tf.assign(var, expanded_ndarray)\n",
    "          sess.run(assign_op)\n",
    "        elif (var_shape[1] != param_shape[1]):\n",
    "          expanded_ndarray = np.ndarray(shape=(param_shape[0], param_shape[1]*2), dtype=float)\n",
    "          expanded_ndarray[:, 0:param_shape[1]] = param_ndarray\n",
    "          expanded_ndarray[:, param_shape[1]:param_shape[1]*2] = param_ndarray\n",
    "          expanded_ndarray /= 2\n",
    "          assign_op = tf.assign(var, expanded_ndarray)\n",
    "          sess.run(assign_op)\n",
    "      elif (var_shape != param_shape and num_dim == 1):\n",
    "        print('      expand')\n",
    "        expanded_ndarray = np.ndarray(shape=(param_shape[0]*2), dtype=float)\n",
    "        expanded_ndarray[0:param_shape[0]] = param_ndarray\n",
    "        expanded_ndarray[param_shape[0]:param_shape[0]*2] = param_ndarray\n",
    "        expanded_ndarray /= 2\n",
    "        assign_op = tf.assign(var, expanded_ndarray)\n",
    "        sess.run(assign_op)\n",
    "      else: #direct copy\n",
    "        print('      direct copy')\n",
    "        assign_op = tf.assign(var, param_ndarray)\n",
    "        sess.run(assign_op)\n",
    "\n",
    "        \n",
    "    print('2. start/resume training')    \n",
    "    # start/resume training\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "      #_, loss_value = sess.run([train_op, loss])\n",
    "      _, loss_value = sess.run([graph1.get_operation_by_name('train'), graph1.get_operation_by_name('total_loss')])\n",
    "      duration = time.time() - start_time\n",
    "        \n",
    "      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "      if step % 10 == 0:\n",
    "        num_examples_per_step = FLAGS.batch_size\n",
    "        examples_per_sec = num_examples_per_step / duration\n",
    "        sec_per_batch = float(duration)\n",
    "\n",
    "        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                      'sec/batch)')\n",
    "        print (format_str % (datetime.now(), step, loss_value,\n",
    "                             examples_per_sec, sec_per_batch))\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "      # Save the model checkpoint periodically.\n",
    "      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir_new, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_path, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "Tensor(\"shuffle_batch:0\", shape=(128, 24, 24, 3), dtype=float32)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x10db1ed10>\n",
      "name: \"total_loss\"\n",
      "op: \"AddN\"\n",
      "input: \"conv1/weight_loss\"\n",
      "input: \"conv2/weight_loss\"\n",
      "input: \"local3/weight_loss\"\n",
      "input: \"local4/weight_loss\"\n",
      "input: \"softmax_linear/weight_loss\"\n",
      "input: \"cross_entropy\"\n",
      "attr {\n",
      "  key: \"N\"\n",
      "  value {\n",
      "    i: 6\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "\n",
      "name: \"train\"\n",
      "op: \"NoOp\"\n",
      "input: \"^GradientDescent\"\n",
      "input: \"^ExponentialMovingAverage\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define a network graph\n",
    "graph1 = tf.Graph()\n",
    "\n",
    "with graph1.as_default():\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "  # Get images and labels for CIFAR-10.\n",
    "  images, labels = cifar10.distorted_inputs()\n",
    "  print(images)\n",
    "  # Build a Graph that computes the logits predictions from the\n",
    "  # inference model.\n",
    "  logits = cifar10.inference(images)\n",
    "\n",
    "  # Calculate loss.\n",
    "  loss = cifar10.loss(logits, labels)\n",
    "\n",
    "  # Build a Graph that trains the model with one batch of examples and\n",
    "  # updates the model parameters.\n",
    "  train_op = cifar10.train(loss, global_step)\n",
    "  \n",
    "  # saver\n",
    "  graph1_saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "print graph1_saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously saved records? (y/n)\n",
      "n\n",
      "Train old or new network? (o/n)\n",
      "n\n",
      "====================================================\n",
      "Train new network\n",
      "====================================================\n",
      "======================extract weights======================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'graph1_saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-854f7404442b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'===================================================='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======================extract weights======================'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph1_saver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#params = extract_weights_old('/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======================train on new network======================'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph1_saver' is not defined"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists(FLAGS.train_dir):\n",
    "  key_in = raw_input('Clear previously saved records? (y/n)\\n')\n",
    "  if (key_in == 'y'):\n",
    "    print(\"Deleting records...\\n\")\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "\n",
    "  # extract weights\n",
    "  key_in = raw_input('Train old or new network? (o/n)\\n')\n",
    "  if (key_in == 'o'):\n",
    "    print('====================================================')\n",
    "    print('Train old network')\n",
    "    print('====================================================')\n",
    "    train()\n",
    "  elif (key_in == 'n'):\n",
    "    print('====================================================')\n",
    "    print('Train new network')\n",
    "    print('====================================================')\n",
    "    print('======================extract weights======================')\n",
    "    params = extract_weights(FLAGS.train_dir_new, graph1_saver)\n",
    "    #params = extract_weights_old('/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train')\n",
    "    print('======================train on new network======================')\n",
    "    #train_load_net(graph1, params, FLAGS.train_dir_new,\n",
    "    #               FLAGS.log_device_placement, FLAGS.max_steps, FLAGS.batch_size,\n",
    "    #               \"train\", \"total_loss\")\n",
    "    train_loaded_net(graph1, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default() :\n",
    "  # Define operations and tensors in `g`.\n",
    "  c = tf.constant(30.0)\n",
    "\n",
    "for op in graph1.get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

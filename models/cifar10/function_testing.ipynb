{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.models.image.cifar10 import cifar10\n",
    "import cifar10\n",
    "import cifar10_mod\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/util')\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "#from extract_weights import extract_weights\n",
    "from Net2Net import Net2Net_class\n",
    "from extract_weights import extract_weights\n",
    "from train_load_net import train_load_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_string('train_dir_new', '/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train_new',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download cifar10 dataset\n",
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph1 = tf.Graph()\n",
    "\n",
    "with graph1.as_default():\n",
    "  global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "  # Get images and labels for CIFAR-10.\n",
    "  images, labels = cifar10_mod.distorted_inputs()\n",
    "\n",
    "  # Build a Graph that computes the logits predictions from the\n",
    "  # inference model.\n",
    "  logits = cifar10_mod.inference(images)\n",
    "\n",
    "  # Calculate loss.\n",
    "  loss = cifar10_mod.loss(logits, labels)\n",
    "\n",
    "  # Build a Graph that trains the model with one batch of examples and\n",
    "  # updates the model parameters.\n",
    "  train_op = cifar10_mod.train(loss, global_step)\n",
    "  \n",
    "  # saver\n",
    "  saver = tf.train.Saver(tf.all_variables())\n",
    "  \n",
    "  # summary\n",
    "  summary_op = tf.merge_all_summaries()\n",
    "\n",
    "  params = extract_weights(FLAGS.train_dir_new, saver)\n",
    "\n",
    "with tf.Session(graph=graph1) as sess:\n",
    "    a = sess.graph.get_operation_by_name('train').outputs\n",
    "    b = sess.graph.get_operation_by_name('total_loss').outputs\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if tf.gfile.Exists(FLAGS.train_dir):\n",
    "  key_in = raw_input('Clear previously saved records? (y/n)\\n')\n",
    "  if (key_in == 'y'):\n",
    "    print(\"Deleting records...\\n\")\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "\n",
    "  # extract weights\n",
    "  key_in = raw_input('Train old or new network? (o/n)\\n')\n",
    "  if (key_in == 'o'):\n",
    "    print('====================================================')\n",
    "    print('Train old network')\n",
    "    print('====================================================')\n",
    "    train()\n",
    "  elif (key_in == 'n'):\n",
    "    print('====================================================')\n",
    "    print('Train new network')\n",
    "    print('====================================================')\n",
    "    print('======================extract weights======================')\n",
    "    \n",
    "    print('======================train on new network======================')\n",
    "    \n",
    "    train_load_net(graph1, params, FLAGS.train_dir_new,\n",
    "                   FLAGS.log_device_placement, FLAGS.max_steps, FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

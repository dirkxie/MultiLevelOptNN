{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.models.image.cifar10 import cifar10\n",
    "import cifar10\n",
    "import cifar10_mod\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/util')\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "#from extract_weights import extract_weights\n",
    "from Net2Net import Net2Net_class\n",
    "from extract_weights import extract_weights\n",
    "from train_load_net import train_load_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_string('train_dir_new', '/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train_new',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download cifar10 dataset\n",
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "1. Checkpoint path found.\n",
      "\n",
      "Load checkpoint state:\n",
      "model_checkpoint_path: \"/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train_new/model.ckpt-0\"\n",
      "all_model_checkpoint_paths: \"/Users/xiejunyi/Dropbox/Research/MultiLevelOptNN/checkpoints/cifar10_train_new/model.ckpt-0\"\n",
      "\n",
      "2. Checkpoint restored.\n",
      "\n",
      "3. Extracted weights info:\n",
      "global_step:0\n",
      "()\n",
      "conv1/weights:0\n",
      "(5, 5, 3, 64)\n",
      "conv1/biases:0\n",
      "(64,)\n",
      "conv2/weights:0\n",
      "(5, 5, 64, 64)\n",
      "conv2/biases:0\n",
      "(64,)\n",
      "local3/weights:0\n",
      "(2304, 768)\n",
      "local3/biases:0\n",
      "(768,)\n",
      "local4/weights:0\n",
      "(768, 192)\n",
      "local4/biases:0\n",
      "(192,)\n",
      "softmax_linear/weights:0\n",
      "(192, 10)\n",
      "softmax_linear/biases:0\n",
      "(10,)\n",
      "conv1/weight_loss/avg:0\n",
      "()\n",
      "conv2/weight_loss/avg:0\n",
      "()\n",
      "local3/weight_loss/avg:0\n",
      "()\n",
      "local4/weight_loss/avg:0\n",
      "()\n",
      "softmax_linear/weight_loss/avg:0\n",
      "()\n",
      "cross_entropy/avg:0\n",
      "()\n",
      "total_loss/avg:0\n",
      "()\n",
      "conv1/weights/ExponentialMovingAverage:0\n",
      "(5, 5, 3, 64)\n",
      "conv1/biases/ExponentialMovingAverage:0\n",
      "(64,)\n",
      "conv2/weights/ExponentialMovingAverage:0\n",
      "(5, 5, 64, 64)\n",
      "conv2/biases/ExponentialMovingAverage:0\n",
      "(64,)\n",
      "local3/weights/ExponentialMovingAverage:0\n",
      "(2304, 768)\n",
      "local3/biases/ExponentialMovingAverage:0\n",
      "(768,)\n",
      "local4/weights/ExponentialMovingAverage:0\n",
      "(768, 192)\n",
      "local4/biases/ExponentialMovingAverage:0\n",
      "(192,)\n",
      "softmax_linear/weights/ExponentialMovingAverage:0\n",
      "(192, 10)\n",
      "softmax_linear/biases/ExponentialMovingAverage:0\n",
      "(10,)\n",
      "[]\n",
      "[<tf.Tensor 'total_loss:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "graph1 = tf.Graph()\n",
    "\n",
    "with graph1.as_default():\n",
    "  global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "  # Get images and labels for CIFAR-10.\n",
    "  images, labels = cifar10_mod.distorted_inputs()\n",
    "\n",
    "  # Build a Graph that computes the logits predictions from the\n",
    "  # inference model.\n",
    "  logits = cifar10_mod.inference(images)\n",
    "\n",
    "  # Calculate loss.\n",
    "  loss = cifar10_mod.loss(logits, labels)\n",
    "\n",
    "  # Build a Graph that trains the model with one batch of examples and\n",
    "  # updates the model parameters.\n",
    "  train_op = cifar10_mod.train(loss, global_step)\n",
    "  \n",
    "  # saver\n",
    "  saver = tf.train.Saver(tf.all_variables())\n",
    "  \n",
    "  # summary\n",
    "  summary_op = tf.merge_all_summaries()\n",
    "\n",
    "  params = extract_weights(FLAGS.train_dir_new, saver)\n",
    "\n",
    "with tf.Session(graph=graph1) as sess:\n",
    "    a = sess.graph.get_operation_by_name('train').outputs\n",
    "    b = sess.graph.get_operation_by_name('total_loss').outputs\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if tf.gfile.Exists(FLAGS.train_dir):\n",
    "  key_in = raw_input('Clear previously saved records? (y/n)\\n')\n",
    "  if (key_in == 'y'):\n",
    "    print(\"Deleting records...\\n\")\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "\n",
    "  # extract weights\n",
    "  key_in = raw_input('Train old or new network? (o/n)\\n')\n",
    "  if (key_in == 'o'):\n",
    "    print('====================================================')\n",
    "    print('Train old network')\n",
    "    print('====================================================')\n",
    "    train()\n",
    "  elif (key_in == 'n'):\n",
    "    print('====================================================')\n",
    "    print('Train new network')\n",
    "    print('====================================================')\n",
    "    print('======================extract weights======================')\n",
    "    \n",
    "    print('======================train on new network======================')\n",
    "    \n",
    "    train_load_net(graph1, params, FLAGS.train_dir_new,\n",
    "                   FLAGS.log_device_placement, FLAGS.max_steps, FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
